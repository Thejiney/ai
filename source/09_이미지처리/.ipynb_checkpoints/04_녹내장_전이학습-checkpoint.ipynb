{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4730137c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 데이터 확보\n",
    "import zipfile\n",
    "filepath = 'data/glaucoma.zip'\n",
    "with zipfile.ZipFile(filepath, 'r') as f:\n",
    "    f.extractall('data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "492cc04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 828 images belonging to 3 classes.\n",
      "Found 320 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "train_generator = ImageDataGenerator(\n",
    "    rotation_range=20, # 최대 회전 각도\n",
    "    width_shift_range=0.2, # 가로 방향 -20% ~ +20% 이동 정도\n",
    "    height_shift_range=0.2, # 상하 방향 이동 정도\n",
    "    horizontal_flip = True, # 수평 뒤집기\n",
    "    vertical_flip = True # 수직 뒤집기\n",
    ")\n",
    "train_data = train_generator.flow_from_directory(\n",
    "    'data/glaucoma/train',\n",
    "    target_size=(224,224),\n",
    "    batch_size=4,\n",
    "    class_mode='sparse', # 원핫인코딩 안 함\n",
    ")\n",
    "test_generator = ImageDataGenerator()\n",
    "test_data = test_generator.flow_from_directory(\n",
    "    'data/glaucoma/test',\n",
    "    target_size=(224,224),\n",
    "    batch_size=4,\n",
    "    class_mode='sparse',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7542abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94765736/94765736 [==============================] - 3s 0us/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resnet50 (Functional)       (None, 7, 7, 2048)        23587712  \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 100352)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              102761472 \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 3075      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 126,352,259\n",
      "Trainable params: 126,299,139\n",
      "Non-trainable params: 53,120\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 2. 모델 구성\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import decode_predictions\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential # ResNet50의 Dense 층을 사용자 추가\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.preprocessing import image # 이미지를 읽어오기\n",
    "\n",
    "resnet_model = ResNet50(input_shape=(224,224,3),\n",
    "                        include_top=False # 컨볼루션과 풀링 레이어만 구조와 가중치를 가져옴\n",
    "                       )\n",
    "resnet_model.trainable = True # ResNet의 가중치도 업데이트 가능\n",
    "# model = Sequential()\n",
    "# model.add(resnet_model)\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(units1024, activation='relu'))\n",
    "# model.add(Dense(units=3, activation='softmax'))\n",
    "model = Sequential([\n",
    "    resnet_model,\n",
    "    Flatten(),\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "model.summary()\n",
    "# 학습 시 필요한 용량\n",
    "# 1. 파라미터 사용 메모리 사용량 : 파라미터수(126,352,259)*4byte / 1024의 제곱 = 482MB\n",
    "# 2. 배치사이즈=3일 경우 3* (224*224*3) * 4byte / 1024의 제곱 = 19MB\n",
    "# 각 layer층 마다 활성화 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf671a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "207/207 [==============================] - 26s 73ms/step - loss: 5.1217 - accuracy: 0.5930 - val_loss: 0.8915 - val_accuracy: 0.6969\n",
      "Epoch 2/20\n",
      "207/207 [==============================] - 13s 65ms/step - loss: 0.7544 - accuracy: 0.6570 - val_loss: 6.6907 - val_accuracy: 0.6406\n",
      "Epoch 3/20\n",
      "207/207 [==============================] - 13s 65ms/step - loss: 1.1055 - accuracy: 0.6341 - val_loss: 32.9456 - val_accuracy: 0.5063\n",
      "Epoch 4/20\n",
      "207/207 [==============================] - 13s 65ms/step - loss: 0.6721 - accuracy: 0.6872 - val_loss: 1.3110 - val_accuracy: 0.6938\n",
      "Epoch 5/20\n",
      "207/207 [==============================] - 13s 64ms/step - loss: 0.6477 - accuracy: 0.6908 - val_loss: 2.4556 - val_accuracy: 0.7031\n",
      "Epoch 6/20\n",
      "207/207 [==============================] - 13s 64ms/step - loss: 0.6053 - accuracy: 0.7331 - val_loss: 2.1122 - val_accuracy: 0.6906\n",
      "Epoch 7/20\n",
      "207/207 [==============================] - 13s 64ms/step - loss: 0.5640 - accuracy: 0.7476 - val_loss: 8.1358 - val_accuracy: 0.5719\n",
      "Epoch 8/20\n",
      "207/207 [==============================] - 13s 65ms/step - loss: 0.5411 - accuracy: 0.7548 - val_loss: 3.3860 - val_accuracy: 0.6875\n",
      "Epoch 9/20\n",
      "207/207 [==============================] - 13s 65ms/step - loss: 0.6252 - accuracy: 0.7403 - val_loss: 1.2557 - val_accuracy: 0.7219\n",
      "Epoch 10/20\n",
      "207/207 [==============================] - 13s 64ms/step - loss: 0.6122 - accuracy: 0.7379 - val_loss: 1.1943 - val_accuracy: 0.6969\n",
      "Epoch 11/20\n",
      "207/207 [==============================] - 13s 64ms/step - loss: 0.5148 - accuracy: 0.7621 - val_loss: 1.9071 - val_accuracy: 0.6812\n",
      "Epoch 12/20\n",
      "207/207 [==============================] - 13s 64ms/step - loss: 0.7624 - accuracy: 0.6993 - val_loss: 1.6878 - val_accuracy: 0.5906\n",
      "Epoch 13/20\n",
      "207/207 [==============================] - 13s 65ms/step - loss: 0.7585 - accuracy: 0.6643 - val_loss: 349.0602 - val_accuracy: 0.3875\n",
      "Epoch 14/20\n",
      "207/207 [==============================] - 13s 65ms/step - loss: 0.8268 - accuracy: 0.7065 - val_loss: 0.9119 - val_accuracy: 0.6719\n",
      "Epoch 15/20\n",
      "207/207 [==============================] - 14s 66ms/step - loss: 0.6539 - accuracy: 0.6908 - val_loss: 1.4301 - val_accuracy: 0.6781\n",
      "Epoch 16/20\n",
      "207/207 [==============================] - 14s 65ms/step - loss: 0.5751 - accuracy: 0.7258 - val_loss: 1.1097 - val_accuracy: 0.6969\n",
      "Epoch 17/20\n",
      "207/207 [==============================] - 14s 65ms/step - loss: 0.5372 - accuracy: 0.7488 - val_loss: 1.6233 - val_accuracy: 0.6000\n",
      "Epoch 18/20\n",
      "207/207 [==============================] - 14s 65ms/step - loss: 0.5265 - accuracy: 0.7572 - val_loss: 1.2472 - val_accuracy: 0.7000\n",
      "Epoch 19/20\n",
      "207/207 [==============================] - 13s 65ms/step - loss: 0.5171 - accuracy: 0.7693 - val_loss: 1.5485 - val_accuracy: 0.7094\n",
      "Epoch 20/20\n",
      "207/207 [==============================] - 13s 65ms/step - loss: 0.5175 - accuracy: 0.7669 - val_loss: 0.8959 - val_accuracy: 0.6250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22ca028d090>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. 학습 설정\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# 4. 학습시키기\n",
    "model.fit(train_data, validation_data=test_data, epochs=20, batch_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4db7c370",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 172800 into shape (224,224,3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 예측하기\u001b[39;00m\n\u001b[0;32m      2\u001b[0m img \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mload_img(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/test.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimg_to_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(x)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(pred)\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 172800 into shape (224,224,3)"
     ]
    }
   ],
   "source": [
    "# 예측하기\n",
    "img = image.load_img('data/test.png')\n",
    "x = image.img_to_array(img).reshape(-1, 224, 224, 3)\n",
    "pred = model.predict(x)\n",
    "print(pred)\n",
    "\n",
    "print(pred.argmax())\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88006b4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
