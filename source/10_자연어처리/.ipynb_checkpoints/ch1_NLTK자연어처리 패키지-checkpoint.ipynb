{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6ffb44a",
   "metadata": {},
   "source": [
    "**<font size=\"5\" color=\"red\">ch1. NLTK 자연어 처리 패키지</font>**\n",
    "# 1. NLTK 패키지\n",
    "1. 텍스트 전처리 : 토큰화(어절, 문장 나누기), 정규표현식을 활용한 토큰화, 불용어제거, 기본형(어근)추출\n",
    "2. 품사태깅 : 단어 품사식별\n",
    "3. 구조화된 문서의 빈도수, 분류분석, 연관분석, 감성분석... (단점)NLTK 속도가 느림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "554aae75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f0ee64",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f3f7dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to d:\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')\n",
    "# c:/nltk_data\n",
    "# d:/nltk_data\n",
    "# e:/nltk_data\n",
    "# c:/Users/내컴퓨터이름/nltk_data\n",
    "# c:/Users/내컴퓨터이름/anaconda3/nltk_data\n",
    "# c:/Users/내컴퓨터이름/anaconda3/share/nltk_data\n",
    "# c:/Users/내컴퓨터이름/anaconda3/lib/nltk_data\n",
    "# c:/Users/내컴퓨터이름/Appdata/Roaming/nltk_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67cff2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fa2edca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 말뭉치 리스트\n",
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5139e406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Emma by Jane Austen 1816]\n",
      "\n",
      "VOLUME I\n",
      "\n",
      "CH\n"
     ]
    }
   ],
   "source": [
    "emma = nltk.corpus.gutenberg.raw('austen-emma.txt')\n",
    "print(emma[:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23e7b7b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "887071"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(emma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ffe5f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "첫문장 : '[Emma by Jane Austen 1816]\\n\\nVOLUME I\\n\\nCHAPTER I\\n\\n\\nEmma Woodhouse, handsome, clever, and rich, with a comfortable home\\nand happy disposition, seemed to unite some of the best blessings\\nof existence; and had lived nearly twenty-one years in the world\\nwith very little to distress or vex her.'\n",
      "두번째 문장 : \"She was the youngest of the two daughters of a most affectionate,\\nindulgent father; and had, in consequence of her sister's marriage,\\nbeen mistress of his house from a very early period.\"\n",
      "문장 수 :  7456\n"
     ]
    }
   ],
   "source": [
    "# sent_tokenize() : 문장단위로 쪼갠 list\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokens = sent_tokenize(emma)\n",
    "print('첫문장 : %r' % (sent_tokens[0]))\n",
    "print('두번째 문장 : %r' % (sent_tokens[1]))\n",
    "print('문장 수 : ', len(sent_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1cbc07fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', 'VOLUME', 'I', 'CHAPTER', 'I', 'Emma', 'Woodhouse', ',', 'handsome', ',', 'clever', ',', 'and', 'rich', ',', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', ',', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessings', 'of', 'existence', ';', 'and', 'had', 'lived', 'nearly', 'twenty-one', 'years', 'in', 'the', 'world', 'with', 'very', 'little', 'to', 'distress', 'or', 'vex', 'her', '.']\n"
     ]
    }
   ],
   "source": [
    "# word_tokenize() : 단어 단위로 쪼갠 list 반환\n",
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(sent_tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a652ada6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Emma', 'by', 'Jane', 'Austen', '1816', 'VOLUME', 'I', 'CHAPTER', 'I', 'Emma', 'Woodhouse', 'handsome', 'clever', 'and', 'rich', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessings', 'of', 'existence', 'and', 'had', 'lived', 'nearly', 'twenty', 'one', 'years', 'in', 'the', 'world', 'with', 'very', 'little', 'to', 'distress', 'or', 'vex', 'her']\n"
     ]
    }
   ],
   "source": [
    "# RegexpTokenizer클래스 : 토큰화할 때 정규표현식이용\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "ret = RegexpTokenizer('\\w+')\n",
    "words = ret.tokenize(sent_tokens[0])\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc572c5d",
   "metadata": {},
   "source": [
    "# 2. 형태소(의미있는 가장 작은 말의 단위)분석\n",
    "- 자연어처리의 기본은 형태소 분석과 품사태깅\n",
    "    * 어간추출(stemming), 원형복원(lemmatizing), 품사태깅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fce0bfdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('send', 'cook', 'file')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = ['sending','cooking', 'files', 'lives','crying','dying']\n",
    "# 어간 추출 방법1\n",
    "from nltk.stem import PorterStemmer\n",
    "pst = PorterStemmer()\n",
    "pst.stem(words[0]), pst.stem(words[1]), pst.stem(words[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e555ce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['send', 'cook', 'file', 'live', 'cri', 'die']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[pst.stem(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6330d708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['send', 'cook', 'fil', 'liv', 'cry', 'dying']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 어간 추출 방법2\n",
    "from nltk.stem import LancasterStemmer\n",
    "lst = LancasterStemmer()\n",
    "[lst.stem(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "03ec391b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['send', 'cook', 'files', 'lives', 'cry', 'dy']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 어간 추출 방법3\n",
    "from nltk.stem import RegexpStemmer\n",
    "rst = RegexpStemmer('ing')\n",
    "[rst.stem(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9cc45701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bel', 'cook']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 어간을 추출할 경우 의미가 달라지는 경우도 있어 원형복원을 하기도 함\n",
    "words2 = ['belives', 'cooking']\n",
    "[lst.stem(word) for word in words2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "14729ece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['belives', 'cooking']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 원형 복원\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    "[wl.lemmatize(word) for word in words2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c0582ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어들 : ['It', 'was', 'on', 'the', 'wedding-day', 'of', 'this', 'beloved', 'friend', 'that', 'Emma', 'first', 'sat', 'in', 'mournful', 'thought', 'of', 'any', 'continuance', '.']\n",
      "\n",
      " 품사태깅결과 : [('It', 'PRP'), ('was', 'VBD'), ('on', 'IN'), ('the', 'DT'), ('wedding-day', 'NN'), ('of', 'IN'), ('this', 'DT'), ('beloved', 'VBN'), ('friend', 'NN'), ('that', 'WDT'), ('Emma', 'NNP'), ('first', 'RB'), ('sat', 'VBD'), ('in', 'IN'), ('mournful', 'JJ'), ('thought', 'NN'), ('of', 'IN'), ('any', 'DT'), ('continuance', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# 품사태깅 : sent_tokens[10]\n",
    "from nltk.tag import pos_tag\n",
    "words = word_tokenize(sent_tokens[10])\n",
    "tagged_list= pos_tag(words)\n",
    "print('단어들 :',words)\n",
    "print('\\n 품사태깅결과 :',tagged_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0319d396",
   "metadata": {},
   "source": [
    "퀴즈 : emma 소설안에서\n",
    "1. 특수문자가 들어가지 않은 3글자 이상의 단어만 추출해서 품사 태깅을 하시오.(RegexpTakenizer)\n",
    "    * emma의 글자수, 단어 출현수, 단어종류, 품사태깅한 데이터 초반 10개 출력\n",
    "2. \"Emma\" 단어가 몇 번 등장하며, 품사 태깅이 어떤 품사들로 되어 있는지 모두 출력하시오(NNP, .. )\n",
    "    * Emma 단어 출현 횟수, 분류된 품사들 NNP, NNPS,...\n",
    "3. 내가 원하는 품사(명사:NN, NNS, NNP, NNPS)의 단어만 뽑아 등장하는 명사의 종류 갯수를 출력하시오\n",
    "    * 명사가 나온 횟수 : 1000\n",
    "    * 출력한 명사의 수(반복제거) : 100\n",
    "    * 한단어가 나오는 평균 빈도수 : 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0bc4848",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Emma', 'NNP'),\n",
       " ('Jane', 'NNP'),\n",
       " ('Austen', 'NNP'),\n",
       " ('1816', 'CD'),\n",
       " ('VOLUME', 'NNP'),\n",
       " ('CHAPTER', 'NNP'),\n",
       " ('Emma', 'NNP'),\n",
       " ('Woodhouse', 'NNP'),\n",
       " ('handsome', 'VBD'),\n",
       " ('clever', 'NN')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer # 3글자 이상의 단어 추출\n",
    "from nltk.tag import pos_tag # 품사 태깅\n",
    "emma = nltk.corpus.gutenberg.raw('austen-emma.txt')\n",
    "\n",
    "ret = RegexpTokenizer('\\w\\w\\w+')\n",
    "words = ret.tokenize(emma)\n",
    "pos_tags=pos_tag(words)\n",
    "pos_tags[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e161c9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emma 단어 출현 횟수 865\n",
      "Index(['NNP', 'NN', 'VB', 'VBP', 'JJ', 'NNS', 'NNPS', 'RB', 'VBN', 'VBD'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#2\n",
    "import pandas as pd\n",
    "df=pd.DataFrame(pos_tags)\n",
    "print('Emma 단어 출현 횟수',len(df[df[0]=='Emma']))\n",
    "print(df[df[0]=='Emma'][1].value_counts().index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a75bdea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "명사가 나온 횟수 30781\n",
      "출력한 명사의 수(반복제거) 4165\n",
      "한단어가 나오는 평균 빈도수 7.3903961584633855\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "df_N=df[(df[1]=='NN')|(df[1]=='NNS')|(df[1]=='NNP')|(df[1]=='NNPS')]\n",
    "print('명사가 나온 횟수',len(df_N[0]))\n",
    "print('출력한 명사의 수(반복제거)',len(set([word for word, tag in pos_tags if tag in ['NN', 'NNS', 'NNP', 'NNPS']])))\n",
    "print('한단어가 나오는 평균 빈도수',len(df_N[0])/len(set([word for word, tag in pos_tags if tag in ['NN', 'NNS', 'NNP', 'NNPS']])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "45098c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 명사들 중 최빈단어 시각화\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import Text\n",
    "# Text : 단어 리스트와 빈도 분서에서 사용될 클래스\n",
    "emma_text = Text(set([word for word, tag in pos_tags if tag in ['NN', 'NNS', 'NNP', 'NNPS']]))\n",
    "plt.figure(figsize=(15,4))\n",
    "emma.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2018ecb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 고유명사(이름) 출현 빈도\n",
    "name_list = [ for]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323c1452",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer # 3글자 이상의 단어 추출\n",
    "from nltk.tag import pos_tag # 품사 태깅\n",
    "emma = nltk.corpus.gutenberg.raw('austen-emma.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2354f1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 특수문자가 들어가지 않은 3글자 이상의 단어만 추출해서 품사 태깅을 하시오(RegexpTokenizer)\n",
    "ret = RegexpTokenizer('[\\w]{3,}')\n",
    "words = ret.tokenize(emma)\n",
    "emma_tags = pos_tag(words)\n",
    "print('emma의 글자수 :', len(emma))\n",
    "print('단어 출현수 :', len(words), len(emma_tags))\n",
    "print('단어 종류수 :', len(set(words)))\n",
    "print(emma_tags[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1ca599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Emma가 몇번 출현하는지? 품사는? (품사 태깅정보 : emma_tags)\n",
    "pos = set()\n",
    "cnt = 0\n",
    "for word, tag in emma_tags:\n",
    "    if word == 'Emma':\n",
    "        cnt += 1\n",
    "        pos.add(tag)\n",
    "print('출현 횟수 :', cnt)\n",
    "print('분류된 품사들 :', pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111cbd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = len([word for word, tag in emma_tags if word=='Emma'])\n",
    "pos = set([tag for word, tag in emma_tags if word=='Emma'])\n",
    "print('출현 횟수 :', cnt)\n",
    "print('분류된 품사들 :', pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eee702e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Emma 단어의 태깅된 품사들의 갯수\n",
    "import pandas as pd\n",
    "pos_cnt = pd.Series([0]*len(pos), index= list(pos))\n",
    "for word, tag in emma_tags:\n",
    "    if word == 'Emma':\n",
    "        pos_cnt[tag] += 1\n",
    "pos_cnt.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf62760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# emma_tags를 데이터 프레임으로 바꿔서 연산\n",
    "tags_df = pd.DataFrame(emma_tags, columns=['word','tag'])\n",
    "pos_cnt = tags_df.loc[tags_df['word']=='Emma','tag']\n",
    "print('Emma 출현 횟수 :', len(pos_cnt))\n",
    "print('분류된 품사들과 출현횟수 :\\n', pos_cnt.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b32d98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 명사들만 추출\n",
    "nouns_list = [emma_t[0] for emma_t in emma_tags if (emma_t[1]=='NN') | (emma_t[1]=='NNS') \n",
    "                        | (emma_t[1]=='NNP') | (emma_t[1]=='NNPS')]\n",
    "\n",
    "nouns_list = [emma_t[0] for emma_t in emma_tags if emma_t[1]=='NN' or emma_t[1]=='NNS' or\n",
    "                        emma_t[1]=='NNP' or emma_t[1]=='NNPS']\n",
    "\n",
    "nouns_list = [word for word, tag in emma_tags if tag in ['NN', 'NNS', 'NNP', 'NNPS']]\n",
    "\n",
    "nouns_list = [word for word, tag in emma_tags if tag[0]=='N']\n",
    "print('명사가 나온 횟수 :', len(nouns_list))\n",
    "print('출현한 명사 수(반복제거) : ', len(set(nouns_list)))\n",
    "print('한단어가 나오는 평균빈도수 : 평균',\n",
    "     round(len(nouns_list) / len(set(nouns_list)),2), '번' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d9f46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 명사들 중 최빈단어 시각화\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import Text\n",
    "# Text : 단어 리스트와 빈도 분석에서 사용될 클래스\n",
    "emma_text = Text(nouns_list)\n",
    "plt.figure(figsize=(15, 4))\n",
    "emma_text.plot(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc208c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "emma_text.dispersion_plot(['Emma','Jane','nothing'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f7621f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 고유명사(이름) 출현 빈도\n",
    "name_list = [word for word, tag in emma_tags if tag in ['NNP','NNPS']]\n",
    "\n",
    "freq_dict = {}\n",
    "for name in name_list:\n",
    "    if name in freq_dict.keys():\n",
    "        freq_dict[name] += 1\n",
    "    else:\n",
    "        freq_dict[name] = 1\n",
    "#print(freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758fdb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "FreqDist(name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44f09cd",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# 150번 이상 반복되는 이름들만 출력\n",
    "for name, freq in FreqDist(name_list).items():\n",
    "    if freq > 150 :\n",
    "        print(name, '-', freq)b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c3089b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위의 내용을 정렬\n",
    "dic = dict(FreqDist(name_list))\n",
    "# dic = freq_dict\n",
    "wordcnt = pd.Series(dic)\n",
    "wordcnt.sort_values(ascending=False, inplace=True)\n",
    "wordcnt.head(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e18fefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 150번 이상 출현한 이름을 정렬해서 출력\n",
    "wordcnt[wordcnt>150]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
